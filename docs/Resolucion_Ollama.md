# Resoluci√≥n del Problema de Ollama - Electridom

## üîç **Diagn√≥stico del Problema**

### Problema Identificado

- **Ollama en modo "low vram"**: Solo 1.9 GiB de memoria disponible de 3.8 GiB total
- **Error 500**: Al intentar descargar modelos desde la API directa
- **Sin GPU compatible**: Usando CPU para inferencia
- **Memoria limitada**: Configuraci√≥n actual no permite descargas

### Estado Actual

- ‚úÖ **Ollama contenedor**: Ejecut√°ndose en puerto 11434
- ‚úÖ **Open WebUI**: Funcionando en puerto 3001
- ‚ùå **Modelos**: No hay modelos descargados
- ‚ùå **Descarga directa**: Falla con error 500

## üõ†Ô∏è **Soluci√≥n Implementada**

### Enfoque: Open WebUI Manual

Dado que la descarga directa falla, hemos implementado un enfoque alternativo usando Open WebUI.

### Scripts Creados

1. **`scripts/configure-ollama.ps1`** - Configuraci√≥n optimizada (ejecutado)
2. **`scripts/download-small-model.ps1`** - Descarga de modelos peque√±os (ejecutado)
3. **`scripts/setup-ollama-minimal.ps1`** - Configuraci√≥n m√≠nima (ejecutado)
4. **`scripts/setup-openwebui.ps1`** - **SOLUCI√ìN ACTUAL** (ejecut√°ndose)

## üìã **Instrucciones para Resolver**

### Paso 1: Configurar Open WebUI

1. **Abrir navegador**: http://localhost:3001
2. **Configuraci√≥n inicial**:
   - Backend URL: `http://localhost:11434`
   - Crear usuario y contrase√±a
   - Guardar configuraci√≥n

### Paso 2: Descargar Modelo

1. **Ir a secci√≥n "Models"**
2. **Buscar modelo peque√±o**:
   - `tinyllama:1b-chat-q4_K_M` (recomendado, ~500MB)
   - `phi3:mini-4k-instruct-q4_K_M` (~1GB)
   - `llama3.1:1b-instruct-q4_K_M` (~1GB)
3. **Descargar modelo**
4. **Esperar completar descarga**

### Paso 3: Verificaci√≥n Autom√°tica

- El script `setup-openwebui.ps1` est√° ejecut√°ndose en segundo plano
- Verifica cada 30 segundos si hay modelos disponibles
- M√°ximo 20 minutos de espera

## üîß **Configuraci√≥n T√©cnica**

### Contenedor Ollama Actual

```bash
Nombre: electridom-ollama-minimal
Configuraci√≥n:
- Memoria limitada: 2GB
- CPUs: 1
- Timeout de carga: 2 minutos
- Modelos m√°ximos: 1
```

### Variables de Entorno

```bash
OLLAMA_HOST=0.0.0.0
OLLAMA_ORIGINS=*
OLLAMA_NUM_PARALLEL=1
OLLAMA_KEEP_ALIVE=1m
OLLAMA_MAX_LOADED_MODELS=1
OLLAMA_LOAD_TIMEOUT=2m
```

## üìä **Estado de Verificaci√≥n**

### Servicios Activos

- ‚úÖ **Ollama**: http://localhost:11434
- ‚úÖ **Open WebUI**: http://localhost:3001
- ‚úÖ **Backend NestJS**: http://localhost:3000
- ‚úÖ **Frontend Angular**: http://localhost:4200

### Verificaci√≥n de Modelos

```bash
# Verificar modelos disponibles
curl http://localhost:11434/api/tags

# Probar modelo (cuando est√© disponible)
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model":"tinyllama:1b-chat-q4_K_M","prompt":"Hola","stream":false}'
```

## üéØ **Pr√≥ximos Pasos**

### Inmediato

1. **Configurar Open WebUI** (manual)
2. **Descargar modelo peque√±o** (manual)
3. **Verificar funcionamiento** (autom√°tico)

### Despu√©s de Modelo Descargado

1. **Probar endpoints de IA**:

   ```bash
   curl http://localhost:3000/api/llm/health
   curl http://localhost:3000/api/llm/provider
   ```

2. **Probar funcionalidades del frontend**:

   - Chat IA: http://localhost:4200/ia/chat
   - C√°lculos IA: http://localhost:4200/ia/calculos

3. **Verificar integraci√≥n completa**

## üö® **Soluci√≥n Alternativa (Si Fallan los Modelos)**

### Opci√≥n 1: Usar OpenAI

- Configurar `OPENAI_API_KEY` en el backend
- Cambiar `LLM_PROVIDER` a "openai"
- Usar OpenAI como fallback

### Opci√≥n 2: Modelo Local Diferente

- Probar modelos a√∫n m√°s peque√±os
- Usar modelos espec√≠ficos para CPU
- Configurar Ollama con menos memoria

### Opci√≥n 3: Optimizaci√≥n de Sistema

- Liberar memoria del sistema
- Cerrar aplicaciones innecesarias
- Reiniciar contenedores Docker

## üìù **Notas Importantes**

1. **Memoria del Sistema**: 16GB total, pero Ollama limitado a ~2GB
2. **GPU**: No disponible, usando CPU
3. **Red**: Conexi√≥n estable para descargas
4. **Docker**: Todos los servicios containerizados

## ‚úÖ **Criterios de √âxito**

- [ ] Modelo descargado en Ollama
- [ ] Respuesta exitosa a prueba de modelo
- [ ] Endpoints de IA funcionando
- [ ] Frontend conectado con backend
- [ ] Chat IA funcional

**El script de verificaci√≥n autom√°tica est√° ejecut√°ndose y notificar√° cuando el modelo est√© disponible.**
